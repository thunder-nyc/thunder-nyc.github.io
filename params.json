{"name":"Thunder","tagline":"C++ 11 Tensor Library","body":"## Introduction\r\n\r\nThunder is a [C++11](http://en.wikipedia.org/wiki/C%2B%2B11) library to provide device-transparent Tensor mathematical operations. We are currently working on a first proof-of-idea version of the library, focusing on applications in [Deep Learning](http://en.wikipedia.org/wiki/Deep_learning) using high-level parallelization and numerical optimization with GPUs and computer clusters.\r\n\r\nThunder is largely inspired by [Torch 7](http://torch.ch), Facebook's [TH++](http://github.com/facebook/thpp), and [EBLearn](http://eblearn.sourceforge.net)'s [libidx](http://eblearn.sourceforge.net/libidx.html). In fact, the name \"Thunder\" came from \"TH--\" if \"under\" could be interpreted as \"--\".\r\n\r\nThe library just had its development began. Contribution is very welcomed! Please contact Xiang Zhang (xiang.zhang [at] nyu.edu) for details.\r\n\r\n## Downloads\r\n\r\n[Current release](https://github.com/thunder-nyc/Thunder/releases) of Thunder is version 0.2.0. You can download it from our [release page](https://github.com/thunder-nyc/Thunder/releases) or check it out in our [Github repository](https://github.com/thunder-nyc/Thunder/tree/v0.2.0).\r\n\r\n## Installation\r\n\r\nHere is a list of prerequisites you need to have to make sure Thunder compiles\r\n* A compiler that supports C++11. Some examples are [gcc](https://gcc.gnu.org) >= 4.8 and [llvm/clang](http://clang.llvm.org) >= 3.4.\r\n\r\n### Compile Thunder\r\n\r\nThunder can be installed via cmake. To configure the project, please execute\r\n```sh\r\n$ cmake\r\n```\r\nThen, you can compile Thunder by\r\n```sh\r\n$ make\r\n```\r\nIf you want to install the project, you can use the following command after cmake configuration.\r\n```sh\r\n$ make install\r\n```\r\n\r\nTo set the installation prefix, you can add an option like `-DCMAKE_INSTALL_PREFIX=/usr/local` to the cmake command.\r\n\r\nUnder Apple OS X you might get a warning regarding [CMP0042](http://www.cmake.org/cmake/help/v3.0/policy/CMP0042.html). Since it is safe to ignore the warning, you can add an option `-Wno-dev` to supress it.\r\n\r\n### Compile Tests\r\n\r\nTo compile tests, you can add an option `-DBUILD_THUNDER_TESTS=ON` to the cmake command. Then, you can run the tests by\r\n```sh\r\n$ make test\r\n```\r\n\r\nIn most systems the tests should be finished with no problems. However, it is possible that you may encounter occasional numerical precision errors depending on your compiler and standard C++ library. Checking the CTest logs manually is needed to make sure everything is okay.\r\n\r\n## Features\r\n\r\nThunder has many exciting features. The following is a preview list. Some of them are already in the current public source code.\r\n\r\n### Device Transparency\r\n\r\nDevice transparency means that we can transfer data between tensors living on different hardware seamlessly. For example\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor cpu_tensor(3, 9, 7, 10);\r\n\r\n// Create a tensor living on NVIDIA GPU and copy from CPU tensor.\r\n// Only explicit static cast is needed.\r\nFloatCudaTensor gpu_tensor = static_cast< FloatCudaTensor >(cpu_tensor);\r\n```\r\n\r\n### Reference Semantics\r\n\r\nTensors in Thunder do not manage memory; rather, they contain [thread-safe C++11 shared pointers](http://en.cppreference.com/w/cpp/memory/shared_ptr) to underlying `Storage` objects. Unless explicitly created by constructors, static tensor creators or a call to `Tensor::clone()` for deep copying, Thunder tensors are light-weight objects that can be copied, moved or returned without heavy side effects.\r\n\r\nThat being said, we still have static memory deallocation when a `Storage` is not linked by anybody. This provides us with both fast Tensor operations and tight memory control without requiring any explicit memory calls by the user.\r\n\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Using tensor constructors create new underlying Storage object.\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Static tensor creators also create new underlying Storage objects.\r\nDoubleTensor created_tensor = DoubleTensor::ones(tensor.size());\r\n\r\n// Copy constructor still points to the same Storage object.\r\nDoubleTensor copied_tensor = tensor;\r\n\r\n// Subtensor operators still points to the same Storage object,\r\n// but now we have a different subtensor view of size 2x8x7x10\r\nDoubleTensor sub_tensor = tensor[{{1,2},{1,8}}]\r\n\r\n// However, the call to 'clone()' creates new underlying Storage.\r\n// It is essentially a 'deep copy'.\r\nDoubleTensor cloned_tensor = tensor.clone();\r\n```\r\n\r\n### Range-based `for` Loop\r\n\r\nWe support the new [C++11 range-based `for` loop](http://en.cppreference.com/w/cpp/language/range-for) on tensors. In Thunder, a range-based `for` loop iterates through the first dimension of the tensor.\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Create a vector of size 10\r\nDoubleTensor vector(10);\r\n\r\n// Create a vector of size 7 storing result data\r\nDoubleTensor result = DoubleTensor::zeros(7);\r\n\r\n// Create a default blas device\r\nDoubleBlas blas_device;\r\n\r\n// Create a default random device\r\nDoubleRandom rand_device;\r\n\r\n// Each t is of size 9x7x10\r\nfor (const DoubleTensor &t : tensor) {\r\n    // Each s is of size 7x10\r\n    for (const DoubleTensor &s : t) {\r\n        // Do matrix-vector multiplication with vector sampled\r\n        // from normal distribution with mean = 0 and std = 1\r\n    \tresult += blas_device.gemv(s, rand_device.normal(vector, 0, 1));\r\n    }\r\n}\r\n```\r\n\r\n### Lambda Expression\r\n\r\nIn Thunder, each tensor can accept a [lambda expression](http://en.cppreference.com/w/cpp/language/lambda) to read or change its values. The following is an example\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Create a value to store the sum\r\ndouble sum = 0;\r\n\r\n// Apply a lambda that sums up the values and assign current sum to current value\r\ntensor.apply(\r\n   [&sum](double v) {\r\n      sum = sum + v;\r\n      return sum;\r\n    });\r\n```\r\n\r\n### Complex Numbers\r\n\r\nThunder library support complex numbers natively.\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create 2 tensors of size 3x9x7x10\r\nDoubleTensor tensor1(3, 9, 7, 10);\r\nDoubleTensor tensor2(3, 9, 7, 10);\r\n\r\n// Construct a complex tensor using polar\r\nDoubleComplexTensor complex_tensor = DoubleComplexTensor::polar(tensor1, tensor2);\r\n\r\n// Get the complex norm of the tensor\r\nDoubleTensor norm_tensor = complex_tensor.getCnrm();\r\n```\r\n\r\n### Serialization\r\n\r\nThunder provides its own serialization functionalities that are very extensible. It can\r\n* Serialize all fundamental types\r\n* Avoid duplicated data saving for pointers\r\n* Track polymorphic types and do correct serialization\r\n* Easily extensible and non-intrusive for classes\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Create a text serializer that serializes to a string\r\nStringTextSerializer string_serializer;\r\n\r\n// Serialize the tensor\r\nstring_serializer.save(tensor);\r\n\r\n// Now you can see the content of the serialized data\r\nprintf(\"Serialized data: %s\\n\", string_serializer.protocol().stream().str().c_str());\r\n```\r\n\r\n### Random Generators\r\n\r\nWe support all [random generators provided by the C++11 standard library](http://en.cppreference.com/w/cpp/numeric/random). They include\r\n* Discrete uniform distribution\r\n* Continuous uniform distribution\r\n* Bernoulli distribution\r\n* Binomial distribution\r\n* Negative binomial distribution\r\n* Geometric distribution\r\n* Poisson distribution\r\n* Exponential distribution\r\n* Gamma distribution\r\n* Weibull distribution\r\n* Extreme value distribution\r\n* Normal distribution\r\n* Log normal distribution\r\n* Chi squared distribution\r\n* Cauchy distribution\r\n* Fisher F distribution\r\n* Student T distribution\r\n\r\nFor example\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a random number generator\r\nDoubleRandom generator;\r\n\r\n// Generate a tensor of size 3x9x7x10 from a gamma distribution\r\n// with alpha = 1.0 and beta = 1.0.\r\nDoubleTensor tensor = generator.gamma({3, 9, 7, 10}, 1.0, 1.0);\r\n```\r\n\r\n### Batch BLAS\r\n\r\nOur BLAS routines support batch mode. The batch mode offers possiblity of speeding up BLAS routines in CPU or GPU without changing the underlying single-core implementation. This design should be more practical and easier to speed up.\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a BLAS computing device\r\nDoubleBlas blas_device;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor1(3, 9, 7, 10);\r\n\r\n// Create another tensor of size 3x9x10\r\nDoubleTensor tensor2(3, 9, 10);\r\n\r\n// Computing matrix-vector multiplication in batch mode\r\n// Now, 'result' is a tensor of size 3x9x7.\r\nDoubleTensor result = blas_device.gemv(tensor1, tensor2);","google":"UA-56015701-1","note":"Don't delete this file! It's used internally to help with page regeneration."}