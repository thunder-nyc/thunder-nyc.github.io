{"name":"Thunder","tagline":"C++ 11 Tensor Library","body":"## Introduction\r\n\r\nThunder is a [C++11](http://en.wikipedia.org/wiki/C%2B%2B11) library to provide device-transparent Tensor mathematical operations. We are currently working on a first proof-of-idea version of the library, focusing on applications in [Deep Learning](http://en.wikipedia.org/wiki/Deep_learning) using high-level parallelization and numerical optimization with GPUs and computer clusters.\r\n\r\nThunder is largely inspired by [Torch 7](http://torch.ch), Facebook's [TH++](http://github.com/facebook/thpp), and [EBLearn](http://eblearn.sourceforge.net)'s [libidx](http://eblearn.sourceforge.net/libidx.html). In fact, the name \"Thunder\" came from \"TH--\" if \"under\" could be interpreted as \"--\".\r\n\r\nThe library just had its development began. Contribution is very welcomed! Please contact Xiang Zhang (xiang.zhang [at] nyu.edu) for details.\r\n\r\n## Downloads\r\n\r\n[Current release](https://github.com/thunder-nyc/Thunder/releases) of Thunder is version 0.0.0. You can download it from our [release page](https://github.com/thunder-nyc/Thunder/releases) or check it out in our [Github repository](https://github.com/thunder-nyc/Thunder/tree/v0.0.0).\r\n\r\n## Installation\r\n\r\nThunder can be installed via a simple make command.\r\n```sh\r\nmake install prefix=/usr/local\r\n```\r\n\r\nPlease note that the Thunder library is still quite new and immature. Many of its features are still under implementation. Current repository contains a development version of Thunder prior to a first release.\r\n\r\n### Prerequisites\r\n\r\nHere is a list of prerequisites you need to have to make sure Thunder compiles\r\n* A C++11 compiler that supports -std=c++11. We target our tests to [gcc](https://gcc.gnu.org) >= 4.8 and [llvm/clang](http://clang.llvm.org) >= 3.4.\r\n* [Boost](http://www.boost.org) >= 1.56, with its [Serialization](http://www.boost.org/doc/libs/1_56_0/libs/serialization) package.\r\n\r\n## Features\r\n\r\nThunder has many exciting features. The following is a preview list. Some of them are already in the current public source code.\r\n\r\n### Device Transparency\r\n\r\nDevice transparency means that we can transfer data between tensors living on different hardware seamlessly. For example\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor cpu_tensor(3, 9, 7, 10);\r\n\r\n// Create a tensor living on NVIDIA GPU and copy from CPU tensor.\r\n// Only explicit static cast is needed.\r\nFloatCudaTensor gpu_tensor = static_cast< FloatCudaTensor >(cpu_tensor);\r\n```\r\n\r\n### Reference Semantics\r\n\r\nTensors in Thunder do not manage memory; rather, they contain [thread-safe C++11 shared pointers](http://en.cppreference.com/w/cpp/memory/shared_ptr) to underlying `Storage` objects. Unless explicitly created by constructors, static tensor creators or a call to `Tensor::clone()` for deep copying, Thunder tensors are light-weight objects that can be copied, moved or returned without heavy side effects.\r\n\r\nThat being said, we still have static memory deallocation when a `Storage` is not linked by anybody. This provides us with both fast Tensor operations and tight memory control without requiring any explicit memory calls by the user.\r\n\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Using tensor constructors create new underlying Storage object.\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Static tensor creators also create new underlying Storage objects.\r\nDoubleTensor created_tensor = DoubleTensor::ones(tensor.size());\r\n\r\n// Copy constructor still points to the same Storage object.\r\nDoubleTensor copied_tensor = tensor;\r\n\r\n// Subtensor operators still points to the same Storage object,\r\n// but now we have a different subtensor view of size 2x8x7x10\r\nDoubleTensor sub_tensor = tensor[{{1,2},{1,8}}]\r\n\r\n// However, the call to 'clone()' creates new underlying Storage.\r\n// It is essentially a 'deep copy'.\r\nDoubleTensor cloned_tensor = tensor.clone();\r\n```\r\n\r\n### Range-based `for` Loop\r\n\r\nWe support the new [C++11 range-based `for` loop](http://en.cppreference.com/w/cpp/language/range-for) on tensors. In Thunder, a range-based `for` loop iterates through the first dimension of the tensor.\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Create a vector of size 10\r\nDoubleTensor vector(10);\r\n\r\n// Create a vector of size 7 storing result data\r\nDoubleTensor result = DoubleTensor::zeros(7);\r\n\r\n// Create a default blas device\r\nDoubleBlas blas_device;\r\n\r\n// Create a default random device\r\nDoubleRandom rand_device;\r\n\r\n// Each t is of size 9x7x10\r\nfor (const DoubleTensor &t : tensor) {\r\n    // Each s is of size 7x10\r\n    for (const DoubleTensor &s : t) {\r\n        // Do matrix-vector multiplication with vector sampled\r\n        // from normal distribution with mean = 0 and std = 1\r\n    \tresult += blas_device.gemv(s, rand_device.normal(vector, 0, 1));\r\n    }\r\n}\r\n```\r\n\r\n### Lambda Expression\r\n\r\nIn Thunder, each tensor can accept a [lambda expression](http://en.cppreference.com/w/cpp/language/lambda) to read or change its values. The following is an example\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Create a value to store the sum\r\ndouble sum = 0;\r\n\r\n// Apply a lambda that sums up the values and assign current sum to current value\r\ntensor.apply(\r\n   [&sum](double v) {\r\n      sum = sum + v;\r\n      return sum;\r\n    });\r\n```\r\n\r\n### Complex Numbers\r\n\r\nThunder library support complex numbers natively.\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create 2 tensors of size 3x9x7x10\r\nDoubleTensor tensor1(3, 9, 7, 10);\r\nDoubleTensor tensor2(3, 9, 7, 10);\r\n\r\n// Construct a complex tensor using polar\r\nDoubleComplexTensor complex_tensor = DoubleComplexTensor::polar(tensor1, tensor2);\r\n\r\n// Get the complex norm of the tensor\r\nDoubleTensor norm_tensor = complex_tensor.getCnrm();\r\n```\r\n\r\n### Serialization\r\n\r\nWe use the [boost serialization](http://www.boost.org/doc/libs/release/libs/serialization) library to serialize all data structures in Thunder.\r\n```cpp\r\nusing namespace thunder;\r\nusing namespace boost;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Create a string stream\r\nstd::stringstream stream;\r\n\r\n// Create an output archive link to the string stream\r\nserialization::text_archive archive(stream);\r\n\r\n// Serialize the tensor to the archive\r\narchive << tensor;\r\n\r\n// Now you can see the content of the serialized data\r\nprintf(\"Serialized data: %s\\n\", stream.str().c_str());\r\n```\r\n\r\n### Random Generators\r\n\r\nWe support all [random generators provided by the C++11 standard library](http://en.cppreference.com/w/cpp/numeric/random). They include\r\n* Uniform distribution\r\n* Bernoulli distribution\r\n* Binomial distribution\r\n* Negative binomial distribution\r\n* Geometric distribution\r\n* Poisson distribution\r\n* Exponential distribution\r\n* Gamma distribution\r\n* Weibull distribution\r\n* Extreme value distribution\r\n* Normal distribution\r\n* Log normal distribution\r\n* Chi squared distribution\r\n* Cauchy distribution\r\n* Fisher F distribution\r\n* Student T distribution\r\n\r\nFor example\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a random number generator\r\nDoubleRandom generator;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor(3, 9, 7, 10);\r\n\r\n// Generate from gamma distribution with alpha = 1.0, beta = 1.0.\r\ngenerator.gamma(1.0, 1.0, &tensor);\r\n```\r\n\r\n### Batch BLAS\r\n\r\nOur BLAS routines support batch mode. The batch mode offers possiblity of speeding up BLAS routines in CPU or GPU without changing the underlying single-core implementation. This design should be more practical and easier to speed up.\r\n```cpp\r\nusing namespace thunder;\r\n\r\n// Create a BLAS computing device\r\nDoubleBlas blas_device;\r\n\r\n// Create a tensor of size 3x9x7x10\r\nDoubleTensor tensor1(3, 9, 7, 10);\r\n\r\n// Create another tensor of size 3x9x10\r\nDoubleTensor tensor2(3, 9, 10);\r\n\r\n// Computing matrix-vector multiplication in batch mode\r\n// Now, 'result' is a tensor of size 3x9x7.\r\nDoubleTensor result = blas_device.gemv(tensor1, tensor2);\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}